# Project Midnight: Domain Adaptive RAW Low-Light Image Enhancement for Smartphone Cameras
**CSE 237D/145 (UCSD Embedded Systems Course) - Spring 2023**

## Team Members
- Vishal Vinod (MS in CSE)
- Shasta Subramanian (BS in ECE)

### [Project Website](https://vishal-vinod.github.io/project-midnight/)

## Abstract
Low-light imaging is a challenging task because of severe noise and low illumination resulting from short-exposure times. Smartphone cameras in particular are low-cost and have higher degradation under darkened environments, with different cameras producing significantly varying images for the same scene. Alleviating this domain gap requires expensive large-scale data capture. To address these drawbacks, we propose to utilize the linear RAW images from a DSLR camera dataset and only a handful of RAW images from a smartphone camera to perform domain adaptive low-light RAW image enhancement. Specifically, we aim to investigate raw-to-sRGB image enhancement, knowledge distillation, and enhanced denoising performance. Project Midnight highlights new insights in utilizing such deep learning techniques to provide an alternative, robust low-light image enhancement implementation. 

## Method
With a noisy raw image captured with low-exposure time (i.e., shutter speed) as input, our CNN-based approach is trained to predict a clean long-exposure sRGB output of the same scene. The input is multiplied by an exposure factor calculated by the ratio of output and input exposure times. For example, to generate a 10-second long exposure output, the input 0.1-second low exposure image must be multiplied by 100. As a result of this operation, along with illumination, the noise is also amplified proportionally. Since we multiply the factor in the unprocessed raw domain and expect the output in the sRGB domain, the network must learn camera hardware-specific enhancement as well as its entire ISP pipeline (lens correction, demosaicing, white balancing, color manipulation, tone curve application, color space transform, and Gamma correction). Thus, a model trained on one specific camera data (source domain) does not translate similar performance to a different camera (target domain), hence the domain gap. In this work, we propose to transfer the enhancement task from large labeled source data and generate output in the target domain using few labeled target data. We also attempt to investigate semi supervised learning using a pseudo ground truth generated by a baseline and utilize the unlabeled samples for improving enhancement performance.

## Reproducible Experimentation
For adapted future research we have also created a [Docker Image](https://hub.docker.com/r/vvinodhub/midnight).

Refer to [LSID](https://github.com/cchen156/Learning-to-See-in-the-Dark) for their codebase and instructions.

## Setup

### Requirement
Required python (version 2.7) libraries: Tensorflow (>=1.1) + Scipy + Numpy + Rawpy.

Tested in Ubuntu + Intel i7 CPU + Nvidia Titan X (Pascal) with Cuda (>=8.0) and CuDNN (>=5.0). CPU mode should also work with minor changes but not tested.

### Dataset

Download it directly from Google drive for [Sony](https://storage.googleapis.com/isl-datasets/SID/Sony.zip) (25 GB)  and [Fuji](https://storage.googleapis.com/isl-datasets/SID/Fuji.zip) (52 GB) sets. 

There is a download limit by Google drive in a fixed period of time. If you cannot download because of this, try these links: [Sony](https://drive.google.com/open?id=1G6VruemZtpOyHjOC5N8Ww3ftVXOydSXx) (25 GB)  and [Fuji](https://drive.google.com/open?id=1C7GeZ3Y23k1B8reRL79SqnZbRBc4uizH) (52 GB).

[UCSD Dataset](https://drive.google.com/drive/folders/1tRrv0ptKCt3Z_tu2sNJ2eliIYG61tohX?usp=sharing)

Note that multiple short-exposed images correspond to the same long-exposed image. 

The file name contains the image information. For example, in "0020_00_0.04s.DNG", "0020" is the image ID; the following "00" is the number in the sequence/burst (00: ground truth, 01: first exposure, 02: second exposure); "0.04s" is the exposure time 1/25 seconds.  

### Training Models
To train the Sony model, run "python train_raw_sony_pixel.py" or "python train_raw_sony_oneplus.py".

By default, the code takes the data in the "./dataset/Sony/" folder and "./dataset/Fuji/".

### Computing Requirements
Loading the raw data and processing by Rawpy takes a lot more time than the backpropagation. The code will load all the groundtruth data processed by Rawpy into memory without 8-bit or 16-bit quantization by default which requires at least 64 GB RAM for training the Sony model and 128 GB RAM for the Fuji model.

## File Descriptions
Within `src`:
* `approach2_mod.py` - 

* `data_ops.py` - 

* `model_jpeg.py` - 

* `run_raw_sony_iphone.sh` - 

* `run_raw_sony_pixel.sh` - 

* `run_raw_sony_oneplus.sh` - 

* `train_raw_sony_pixel.py`- 

* `train_sony_oneplus.py` - 

All files within `static` are for the [Project Website](https://vishal-vinod.github.io/project-midnight/)

## Literature Survey
Our preliminary study to validate our findings will be to explore the current state-of-the-art RAW low-light image enhancement techniques and the available datasets. Low-light image enhancement has received significant attention from the computational photography literature, yet only recently has there been branches of research investigating RAW camera sensor data for this task. Further, there have been very few works that investigate cross-camera image enhancement or camera-agnostic denoising techniques. We provide a brief overview of previous work that we explored during our literature survey to potentially fine-tune our research goals:

Several previous work primarily enhance low-light sRGB images to well-illuminated sRGB images  requiring large paired datasets. These methods inherently expect the noise model to be minimal in order to enhance the image and thus have sub-par performance under severe degradation that varies with each camera sensor. Recent methods such as DeepUPE, KnD++, and Zero-DCE show promising image enhancement performance for low-light scenes from the LoL dataset but have not performed well in extreme low-light for RAW images. Attention map based method, attention aggregation method and HDR imaging method have shown promising low-light enhancement performance for sRGB images. LSID proposes a deep learning based method to learn the entire non-linear camera pipeline in an end-to-end manner capable of enhancing RAW images for a single camera dataset. Another technique for image enhancement proposes to decompose the RAW image into the frequency domain to recover information from the low frequency domain and then enhance the details. This is a very different task different from the task of cross-camera adaptive RAW image enhancement. More recently, FSDA-LL proposes a few-shot domain adaptive method for low-light RAW enhancement but do not investigate in detail the enhancement for smartphone cameras or the camera-specific denoising. In this work, we investigate these shortcomings and further experiment with different ranges of exposure ratios for the same scene captured with three different smartphone cameras.

### Key References
- Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun, "Learning to See in the Dark", in CVPR, 2018.
- Prabhakar, V. Vinod, N. Ranjan, and V. B. Radhakrishnan. Few-shot domain adaptation for low light raw image enhancement. In BMVC, 2021.