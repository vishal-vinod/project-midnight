<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project Midnight: Domain Adaptive RAW Low-Light Image Enhancement for Smartphone Cameras">
  <meta property="og:title" content="Midnight"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/fsdall.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Midnight">
  <meta name="twitter:description" content="Project Midnight: Domain Adaptive RAW Low-Light Image Enhancement for Smartphone Cameras">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/fsdall.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Project Midnight">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Project Midnight: Domain Adaptive RAW Low-Light Image Enhancement for Smartphone Cameras</title>
  <link rel="icon" type="image/x-icon" href="static/images/fsdall.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Project Midnight: Domain Adaptive RAW Low-Light Image Enhancement for Smartphone Cameras</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://vishal-v.github.io/" target="_blank">Vishal Vinod</a>,</span>
              <span class="author-block">
                <a href="" target="">Shasta Subramanian</a></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">UC San Diego<br/>{vvinod, s1subram}@ucsd.edu</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                   <!-- <span class="link-block">
                     <a href="" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                     <span class="icon">
                       <i class="fas fa-file-pdf"></i>
                     </span>
                     <span>Supplementary</span>
                   </a>
                 </span> -->

                  <!--Data link  -->
                 <span class="link-block">
                     <a href="https://drive.google.com/drive/folders/1oCPHfsWFGz_MfmoQlzti6Yp92L2LIfMi?usp=sharing" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                     <span class="icon">
                       <i class="fas fa-images"></i>
                     </span>
                     <span>Dataset</span>
                   </a>
                 </span>

                  <!-- Github link -->
                 <span class="link-block">
                   <a href="https://github.com/vishal-vinod/project-midnight" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                     <i class="fab fa-github"></i>
                   </span>
                   <span>Code</span>
                 </a>
               </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2303.15528" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- <h3 class="title is-4">Texture Transfer</h3> -->
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="results" class="center">
      <h2 class="subtitle has-text-centered">
    </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Low-light imaging is a challenging task, especially when images are captured using a smartphone camera. This is because 
            of severe noise and low illumination resulting from the short-exposure time used in image capture. Additionally, smartphone 
            camera sensors are low-cost and have higher degradation under such conditions and each camera can produce significantly 
            different images for the same scene leading to a domain gap. Alleviating this domain gap requires expensive large-scale 
            data capture. To address these challenges, we propose to utilize the linear RAW images from a DSLR camera dataset and 
            only a handful of RAW images from a smartphone camera to perform domain adaptive low-light RAW image enhancement. 
            Specifically, we aim to investigate raw-to-sRGB image enhancement, knowledge distillation, and enhanced denoising 
            performance. Below we have reported our milestone updates thus far including our plan for successful execution for 
            the remaining tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <img src="static/images/RAW-RAW.png" alt="teaser" class="center">
        <div class="content has-text-justified">
          With a noisy raw image captured with low-exposure time (i.e., shutter speed) as input, our CNN-based approach is trained to predict a 
          clean long-exposure sRGB output of the same scene. The input is multiplied by an exposure factor calculated by the ratio of output and 
          input exposure times. For example, to generate a 10-second long exposure output, the input 0.1-second low exposure image must be 
          multiplied by 100. As a result of this operation, along with illumination, the noise is also amplified proportionally. Since we multiply 
          the factor in the unprocessed raw domain and expect the output in the sRGB domain, the network must learn camera hardware-specific 
          enhancement as well as its entire ISP pipeline (lens correction, demosaicing, white balancing, color manipulation, tone curve application, 
          color space transform, and Gamma correction). Thus, a model trained on one specific camera data (source domain) does not translate similar 
          performance to a different camera (target domain), hence the domain gap. In this work, we propose to transfer the enhancement task from 
          large labeled source data and generate output in the target domain using few labeled target data. We also attempt to investigate semi supervised
          learning using a pseudo ground truth generated by a baseline and utilize the unlabeled samples for improving enhancement performance.
        <br/><br/>
        <img src="static/images/semi-supervised.png" alt="teaser" class="center">
        <br/>
        </div>
        <h2 class="title is-3">Smartphone Datasets</h2>
        <div class="content has-text-justified">
        <img src="static/images/Midnight-data.png" alt="teaser" class="center">
        <!-- <div class="content has-text-justified"> -->
          Example short-exposure and long-exposure image pairs from the captured smartphone datasets. The short exposure images are almost entirely dark whereas 
          the long-exposure images contain scene information.
        <br/>
        </div>
        <div class="content has-text-justified">
          We have compiled a dataset of raw low-light images captured with a Google Pixel 6a, iPhone 11 and OnePlus Nord smartphone camera to train the proposed few-shot domain adaptation 
          architecture. Each dataset consists of short-exposure images and corresponding long exposure data captured in the .DNG format. For uniformity, there are two short-exposure 
          images for every long-exposure image such that the exposure ratio (ratio of exposure time between the ground-truth long-exposure image and 
          the input short-exposure image) is between 100 and 300, respectively. Similar to LSID and FSDA-LL, we mount the camera on a sturdy tripod and 
          use appropriate camera settings to capture the static scenes using a smartphone app. The distribution of images captured are included in the following table:
        <br/><br/>
          <img src="static/images/table.png" alt="teaser" class="center">
        <br/>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>

        <!-- <h3 class="title is-4">Sony as Source and Nikon as Target</h3>
        <img src="static/images/Nikon-Results-1_1.png" alt="results" class="center">
        <div class="content has-text-centered">
        </div> -->

        <!-- <img src="static/images/Nikon-3.png" alt="results" class="center">
        <div class="content has-text-centered">
        </div>

        <br/> -->

        <!-- <h3 class="title is-4">Sony as Source and Canon as Target</h3>
        <img src="static/images/Canon-Results-1.png" alt="results" class="center">
        <div class="content has-text-centered"> -->
          <!-- <br>Qualitative comparison between different methods tested on Canon target images. -->
        <!-- </div> -->

        <!-- <h3 class="title is-4">Canon dataset </h3> -->
        <!-- <img src="static/images/Canon-2.png" alt="results" class="center"> -->
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Canon target images. -->
        </div>

        <!-- <img src="static/images/Canon-3.png" alt="results" class="center">
        <div class="content has-text-centered">
          <br>Qualitative comparison between different methods tested on Canon target images.
        </div> -->

        <br/>

        <h3 class="title is-4">Sony as Source and Pixel 6a as Target</h3>
        <img src="static/images/OnePlus-1.png" alt="results" class="center">
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Canon target images. -->
        </div>

        <br/>

        <h3 class="title is-4">Sony as Source and iPhone 11 as Target</h3>
        <img src="static/images/Pixel-1.png" alt="results" class="center">
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Canon target images. -->
        </div>

      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->

<!--BibTex citation -->

<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
